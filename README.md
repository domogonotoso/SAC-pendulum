í”„ë ˆì„ë³„ reward/step/epsilon í‘œì‹œ, make gif and put it at readme

## Pendulum
![asdf](results/videos/annotated_episode_dynamic.gif)
## SAC (Soft Actor Critic), What is it?
Soft Actor-Critic (SAC) is a model-free, off-policy RL algorithm for continuous action spaces.  
It uses an actor network to output mean and standard deviation of actions, creating a Gaussian policy.  
SAC also includes entropy regularization to balance exploration and exploitation, making the policy soft. 
![alt text]



## ğŸ“ Project Structure

```text
sac-pendulum/
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ sac_agent.py         # SAC agent with actor-critic update logic and replay buffer
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ actor.py             # Actor network definition
â”‚   â”œâ”€â”€ critic.py            # Critic (Q-value) network definition
â”‚   â””â”€â”€ value.py             # (Optional) Value network if using soft value update separately
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ plot.py              # Training curve plotting utilities
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sac_config.yaml      # Hyperparameter configuration file
â”œâ”€â”€ videos/                  # Recorded environment videos
â”‚   â””â”€â”€ rl-video-episode-0.mp4
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ rewards_plot.png     # Reward curve plot
â”‚   â””â”€â”€ saved_model.pth      # Final trained model checkpoint
â”œâ”€â”€ main.py                  # Entry point for training/eval/render
â”œâ”€â”€ train.py                 # Training loop
â”œâ”€â”€ test.py                  # Evaluation script
â”œâ”€â”€ requirements.txt         # Required packages
â”œâ”€â”€ README.md                # Project overview and usage
â””â”€â”€ .gitignore               # Common ignores (pycache, videos, etc.)


```





! [reward_plot](results/rewards_plot.png)