## Pendulum
![asdf](results/videos/annotated_episode_dynamic.gif)
  ## SAC (Soft Actor Critic), What is it?
Soft Actor-Critic (SAC) is off-policy RL algorithm for continuous action spaces.  
It uses an actor network to output mean and standard deviation of actions, creating a Gaussian policy.  
SAC also includes entropy regularization to balance exploration and exploitation, making the policy soft. 
![alt text]

https://www.youtube.com/watch?v=cy8r7WSuT1I&ab_channel=3Blue1Brown

## ğŸ“ Project Structure

```text
sac-pendulum/
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ sac_agent.py         # SAC agent with actor-critic update logic and replay buffer
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ actor.py             # Actor network definition
â”‚   â”œâ”€â”€ critic.py            # Critic (Q-value) network definition
â”‚   â””â”€â”€ value.py             # (Optional) Value network if using soft value update separately
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ plot.py              # Training curve plotting utilities
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sac_config.yaml      # Hyperparameter configuration file
â”œâ”€â”€ videos/                  # Recorded environment videos
â”‚   â””â”€â”€ rl-video-episode-0.mp4
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ rewards_plot.png     # Reward curve plot
â”‚   â””â”€â”€ saved_model.pth      # Final trained model checkpoint
â”œâ”€â”€ main.py                  # Entry point for training/eval/render
â”œâ”€â”€ train.py                 # Training loop
â”œâ”€â”€ test.py                  # Evaluation script
â”œâ”€â”€ requirements.txt         # Required packages
â”œâ”€â”€ README.md                # Project overview and usage
â””â”€â”€ .gitignore               # Comon ignores (pycache, videos, etc.)


```





![reward_plot](results/rewards_plot.png)



log_std



load model and even parameters of each Adam optimizer from pth

log_prob ì´ê±° ì—”íŠ¸ë¡œí”¼ êµ¬í•˜ëŠ”ê±° ê°™ì€ë°. ì™œ tanh actionbound í•˜ê¸° ì „ì— í•˜ëŠ”ê±°ì§€? ì•„ë‹ˆ ì• ì´ˆì— action ê°’ë‚˜ì˜¨ê²Œ action bound ì•ˆì— ì•ˆë“œë„¤?
ê·¸ëŸ¼ ì•ˆë“œëŠ” ë°ì´í„°, ì²˜ìŒì— ëª¨ë¸ì—ì„œ ë‚˜ì˜¨ ë°ì´í„°ë¡œë§Œ ì—”íŠ¸ë¡œí”¼ êµ¬í•´ì•¼ ì •í™•íˆ êµ¬í•´ì§€ëŠ”ê±´ê°€



F.softplus(-2 * u)  # log(1 + exp(-2u))



## implement
1. block overestimating q-value
```python
        # sac_agent.py, line 97
        q1_pi = self.critic_1(state, action_sample)
        q2_pi = self.critic_2(state, action_sample)
        min_q_pi = torch.min(q1_pi, q2_pi)
```
2. Soft update
```python
        # sac_agent.py, line 107
        for target_param, param in zip(self.critic_1_target.parameters(), self.critic_1.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
        for target_param, param in zip(self.critic_2_target.parameters(), self.critic_2.parameters()):
            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)
```
3. video with step and reward per frames
4. main.py  train test render




## Curious í—·ê°ˆë ¸ë˜ê±°

```python
        log_prob = dist.log_prob(action_sample).sum(dim=-1, keepdim=True)
        log_prob -= (2 * (np.log(2) - action_sample - F.softplus(-2 * action_sample))).sum(dim=-1, keepdim=True)
```
ì—”íŠ¸ë¡œí”¼: $H(\pi) = -\mathbb{E}_{a \sim \pi}[\log \pi(a|s)]$

$$
-\log(1 - \tanh^2(x)) = 2(\log(2) - x - \mathrm{softplus}(-2x))
$$
